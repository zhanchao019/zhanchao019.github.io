---
layout: default
title: Demo
---

<div class="post">
	<h2 class="pageTitle">Demo:</h2>
	<h2 class="pageTitle">my blog page</h2>

<p><font size=5>MuseV: Infinite-length and High Fidelity Virtual Human Video Generation with Visual Conditioned  Parallel Denoising
</br>
Zhiqiang Xia <sup>*</sup>,
Zhaokang Chen<sup>*</sup>,
Bin Wu<sup>†</sup>,
Chao Li,
Kwok-Wai Hung,
Chao Zhan,
Yingjie He,
Wenjiang Zhou
(<sup>*</sup>co-first author, <sup>†</sup>Corresponding Author, benbinwu@tencent.com)
</font></p>

<p><strong><a href="https://github.com/TMElyralab/MuseV">github</a></strong>    <strong><a href="https://huggingface.co/TMElyralab/MuseV">huggingface</a></strong>  <strong><a href="https://huggingface.co/spaces/AnchorFake/MuseVDemo">HuggingfaceSpace</a></strong>  <strong><a href="#">project (coming soon)</a></strong>    <strong>Technical report (coming soon)</strong></p>

<p>We have setup <strong>the world simulator vision since March 2023, believing diffusion models can simulate the world</strong>. <code>MuseV</code> was a milestone achieved around <strong>July 2023</strong>. Amazed by the progress of Sora, we decided to open-source <code>MuseV</code>, hopefully, it will benefit the community. Next, we will move on to the promising diffusion+transformer scheme.</p>

<p>We will soon release <code>MuseTalk</code>, a real-time high-quality lip sync model, which can be applied with MuseV as a complete virtual human generation solution. Please stay tuned!</p>

<h1>What is MuseV</h1>
<p><code>MuseV</code> is a diffusion-based virtual human video generation framework, which</p>
<ol>
  <li>supports <strong>infinite length</strong> generation using a novel <strong>Visual Conditioned Parallel Denoising scheme</strong>.</li>
  <li>checkpoint available for virtual human video generation trained on human dataset.</li>
  <li>supports Image2Video, Text2Image2Video, Video2Video.</li>
  <li>compatible with the <strong>Stable Diffusion ecosystem</strong>, including <code>base_model</code>, <code>lora</code>, <code>controlnet</code>, etc.</li>
  <li>supports multi-reference image technology, including <code>IPAdapter</code>, <code>ReferenceOnly</code>, <code>ReferenceNet</code>, <code>IPAdapterFaceID</code>.</li>
  <li>training codes (coming very soon).</li>
</ol>

<h1>News</h1>
<ul>
  <li>[03/27/2024] release <code>MuseV</code> project and trained model <code>musev</code>, <code>muse_referencenet</code>.</li>
  <li>[03/30/2024] add huggingface space gradio to generate video in gui</li>
</ul>

</body>
</html>
</div>

---
layout: default
title: MuseV Project Page
---

<div class="post">
<p><font size=5>
</br>
Zhiqiang Xia <sup>*</sup>,
Zhaokang Chen<sup>*</sup>,
Bin Wu<sup>†</sup>,
Chao Li,
Kwok-Wai Hung,
Chao Zhan,
Yingjie He,
Wenjiang Zhou
(<sup>*</sup>co-first author, <sup>†</sup>Corresponding Author, benbinwu@tencent.com)
</font></p>

<p><strong><a href="https://github.com/TMElyralab/MuseV">Github</a></strong>    <strong><a href="https://huggingface.co/TMElyralab/MuseV">Huggingface</a></strong>  <strong><a href="https://huggingface.co/spaces/AnchorFake/MuseVDemo">HuggingfaceSpace</a></strong>  <strong>Technical report (coming soon)</strong></p>


<h1>News</h1>
<ul>
  <li>[03/27/2024] release <code>MuseV</code> project and trained model <code>musev</code>, <code>muse_referencenet</code>.</li>
  <li>[03/30/2024] add huggingface space gradio to generate video in gui</li>
</ul>

<h1>What is MuseV</h1>
<p><code>MuseV</code> is a diffusion-based virtual human video generation framework, which</p>
<ol>
  <li>supports <strong>infinite length</strong> generation using a novel <strong>Visual Conditioned Parallel Denoising scheme</strong>.</li>
  <li>checkpoint available for virtual human video generation trained on human dataset.</li>
  <li>supports Image2Video, Text2Image2Video, Video2Video.</li>
  <li>compatible with the <strong>Stable Diffusion ecosystem</strong>, including <code>base_model</code>, <code>lora</code>, <code>controlnet</code>, etc.</li>
  <li>supports multi-reference image technology, including <code>IPAdapter</code>, <code>ReferenceOnly</code>, <code>ReferenceNet</code>, <code>IPAdapterFaceID</code>.</li>
  <li>training codes (coming very soon).</li>
</ol>

</body>
</html>
</div>
